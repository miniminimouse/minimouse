import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple, Union
from datetime import datetime
import warnings

def rolling_mae(
    datasets: List[pd.DataFrame],
    frequency: str,
    excluded_quarters: Optional[List[str]] = None,
    rolling_periods: int = 4,
    date_col: str = 'date',
    column_mapping: Optional[Dict[int, Dict[str, str]]] = None,
    all_cols: Optional[Dict[str, str]] = None
) -> Dict[int, Dict[str, Union[float, pd.Series]]]:
    """
    Calculate rolling MAE with warning and breach intervals for model validation.
    
    This function computes development MAE metrics used in banking model validation
    frameworks like CCAR and CECL. It calculates rolling MAE over specified periods
    and determines statistical thresholds for model performance monitoring.
    
    Parameters:
    -----------
    datasets : List[pd.DataFrame]
        List of datasets to analyze. Each should contain date, actual, and predicted columns.
    frequency : str
        Data frequency - 'month', 'quarter', or 'year'
    excluded_quarters : List[str], optional
        Quarters to exclude in format "1q2008", "2q2009", etc.
    rolling_periods : int, default=4
        Number of periods for rolling calculation (typically 4 quarters)
    date_col : str, default='date'
        Name of date column (format: "yyyy-mm-dd")
    column_mapping : Dict[int, Dict[str, str]], optional
        Dataset-specific column mapping. Format:
        {0: {'actual_col': 'actual_loss', 'predicted_col': 'pred_loss'},
         1: {'actual_col': 'true_val', 'predicted_col': 'forecast'}}
    all_cols : Dict[str, str], optional
        Column names to use for all datasets. Format:
        {'actual_col': 'actual_var', 'predicted_col': 'predicted_var'}
        Ignored if column_mapping is provided.
        
    Returns:
    --------
    Dict[int, Dict[str, Union[float, pd.Series]]]
        Dictionary with dataset index as key, containing:
        - 'development_mae': float - Average of rolling MAE
        - 'warning_interval': float - Mean + 1 std dev
        - 'breach_interval': float - Mean + 2 std dev
        - 'rolling_mae_series': pd.Series - Complete rolling MAE time series
        - 'excluded_periods': List[str] - Periods that were excluded
    """
    
    if not datasets:
        raise ValueError("At least one dataset must be provided")
    
    if frequency.lower() not in ['month', 'quarter', 'year']:
        raise ValueError("Frequency must be 'month', 'quarter', or 'year'")
    
    if rolling_periods < 1:
        raise ValueError("Rolling periods must be at least 1")
    
    # Handle column mapping logic
    if column_mapping is None and all_cols is None:
        # Default fallback
        all_cols = {'actual_col': 'actual_var', 'predicted_col': 'predicted_var'}
    
    excluded_quarters = excluded_quarters or []
    results = {}
    
    for idx, df in enumerate(datasets):
        try:
            # Determine column names for this dataset
            if column_mapping and idx in column_mapping:
                actual_col = column_mapping[idx]['actual_col']
                predicted_col = column_mapping[idx]['predicted_col']
            elif all_cols:
                actual_col = all_cols['actual_col']
                predicted_col = all_cols['predicted_col']
            else:
                raise ValueError(f"No column mapping found for dataset {idx}")
            
            # Basic validation
            if df.empty:
                warnings.warn(f"Dataset {idx} is empty, skipping...")
                continue
                
            required_cols = [date_col, actual_col, predicted_col]
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                raise ValueError(f"Dataset {idx} missing columns: {missing_cols}")
            
            # Work with a copy to avoid modifying original
            data = df.copy()
            
            # Ensure date column is datetime
            data[date_col] = pd.to_datetime(data[date_col])
            data = data.sort_values(date_col).reset_index(drop=True)
            
            # Create quarter identifier for exclusions
            data['quarter_id'] = data[date_col].dt.to_period('Q').astype(str).str.replace('Q', 'q').str.lower()
            
            # Filter out excluded quarters
            excluded_periods = []
            if excluded_quarters:
                mask = ~data['quarter_id'].isin([q.lower() for q in excluded_quarters])
                excluded_count = len(data) - mask.sum()
                if excluded_count > 0:
                    excluded_periods = data[~mask]['quarter_id'].unique().tolist()
                    data = data[mask].reset_index(drop=True)
            
            if len(data) < rolling_periods:
                warnings.warn(f"Dataset {idx} has insufficient data points ({len(data)}) for rolling calculation of {rolling_periods} periods")
                continue
            
            # Calculate MAE for each period
            data['mae'] = np.abs(data[actual_col] - data[predicted_col])
            
            # Handle different frequencies for rolling calculation
            if frequency.lower() == 'quarter':
                # Group by quarter and calculate mean MAE per quarter
                data['period'] = data[date_col].dt.to_period('Q')
                quarterly_mae = data.groupby('period')['mae'].mean()
                rolling_mae_series = quarterly_mae.rolling(window=rolling_periods, min_periods=rolling_periods).mean()
            elif frequency.lower() == 'month':
                # Group by month
                data['period'] = data[date_col].dt.to_period('M')
                monthly_mae = data.groupby('period')['mae'].mean()
                rolling_mae_series = monthly_mae.rolling(window=rolling_periods, min_periods=rolling_periods).mean()
            else:  # year
                # Group by year
                data['period'] = data[date_col].dt.to_period('Y')
                yearly_mae = data.groupby('period')['mae'].mean()
                rolling_mae_series = yearly_mae.rolling(window=rolling_periods, min_periods=rolling_periods).mean()
            
            # Remove NaN values from rolling calculation
            rolling_mae_clean = rolling_mae_series.dropna()
            
            if len(rolling_mae_clean) == 0:
                warnings.warn(f"Dataset {idx} produced no valid rolling MAE values")
                continue
            
            # Calculate development metrics
            development_mae = rolling_mae_clean.mean()
            rolling_std = rolling_mae_clean.std()
            
            # Calculate intervals - these are upper bounds for performance degradation
            warning_interval = development_mae + (1 * rolling_std)
            breach_interval = development_mae + (2 * rolling_std)
            
            # Store results
            results[idx] = {
                'development_mae': development_mae,
                'warning_interval': warning_interval,
                'breach_interval': breach_interval,
                'rolling_mae_series': rolling_mae_clean,
                'excluded_periods': excluded_periods,
                'data_points_used': len(data),
                'rolling_periods_calculated': len(rolling_mae_clean)
            }
            
        except Exception as e:
            warnings.warn(f"Error processing dataset {idx}: {str(e)}")
            continue
    
    # Print results automatically
    if results:
        print("=" * 70)
        print("ROLLING MAE VALIDATION RESULTS")
        print("=" * 70)
        
        for dataset_idx, metrics in results.items():
            dev_mae = metrics['development_mae']
            warn_thresh = metrics['warning_interval']
            breach_thresh = metrics['breach_interval']
            
            print(f"\nDataset {dataset_idx}:")
            print(f"  Development MAE: {dev_mae:.6f}")
            print(f"  Good interval: MAE < {warn_thresh:.6f}")
            print(f"  Warning: {warn_thresh:.6f} <= MAE < {breach_thresh:.6f}")
            print(f"  Breach: MAE >= {breach_thresh:.6f}")
            print(f"  Data points used: {metrics['data_points_used']}")
            print(f"  Rolling calculations: {metrics['rolling_periods_calculated']}")
            
            if metrics['excluded_periods']:
                print(f"  Excluded periods: {', '.join(metrics['excluded_periods'])}")
    else:
        print("No valid results to display - check your data and parameters")
    
    return results



# Example usage and testing
if __name__ == "__main__":
    # Generate sample data for testing
    np.random.seed(42)
    
    # Create sample dataset
    dates = pd.date_range('2020-01-31', '2023-12-31', freq='M')
    n_points = len(dates)
    
    # Simulate actual and predicted values with some realistic noise
    actual_vals = 100 + np.cumsum(np.random.normal(0, 5, n_points))
    predicted_vals = actual_vals + np.random.normal(0, 3, n_points)  # Some prediction error
    
    sample_data = pd.DataFrame({
        'date': dates,
        'actual_var': actual_vals,
        'predicted_var': predicted_vals
    })
    
    # Test the function with different column mapping approaches
    
    # Method 1: Using all_cols for same column names across datasets
    print("Testing with all_cols parameter:")
    test_results1 = rolling_mae(
        datasets=[sample_data],
        frequency='quarter',
        excluded_quarters=['1q2020', '2q2020'],
        rolling_periods=4,
        all_cols={'actual_col': 'actual_var', 'predicted_col': 'predicted_var'}
    )
    
    # Method 2: Using column_mapping for dataset-specific columns
    print("\n" + "="*50)
    print("Testing with column_mapping parameter:")
    
    # Create a second dataset with different column names
    sample_data2 = sample_data.copy()
    sample_data2.rename(columns={'actual_var': 'true_value', 'predicted_var': 'forecast'}, inplace=True)
    
    test_results2 = rolling_mae(
        datasets=[sample_data, sample_data2],
        frequency='quarter',
        excluded_quarters=['1q2020', '2q2020'],
        rolling_periods=4,
        column_mapping={
            0: {'actual_col': 'actual_var', 'predicted_col': 'predicted_var'},
            1: {'actual_col': 'true_value', 'predicted_col': 'forecast'}
        }
    )
